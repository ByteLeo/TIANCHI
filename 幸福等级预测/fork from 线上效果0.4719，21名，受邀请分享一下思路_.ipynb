{"cells":[{"cell_type":"code","source":"线上效果0.4719，21名，受邀请分享一下思路~\n\n总体思路：分别使用LightGBM，xgboost，gbdt，catboost建立多个个体学习器（加入bagging的策略，对数据随机采样），对最终学习器的输出使用岭回归进一步提升精度。代码如下。\n\n改进点：\n1.可以在详细分析一下字段，可以考虑对字段进行特殊处理。\n2.超参数还可以调，我没有使用网格搜索，只是简单的进行的调参。\n3.如果单纯为了提高精度，可以更高随机种子，多试几次\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#28名 LGBMRegressor\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"happiness_train_complete.csv\",encoding=\"GB2312\")\ndf = df.sample(frac=1,replace=False,random_state=11)\ndf.reset_index(inplace=True)\ndf = df[df[\"happiness\"]>0]\nY = df[\"happiness\"]\ndf[\"survey_month\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\ndf[\"survey_day\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\ndf[\"survey_hour\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\nX = df.drop(columns=[\"id\",\"index\",\"happiness\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\n\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm.sklearn import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=15, shuffle = True, random_state= 12)\nmodel = LGBMRegressor(n_jobs=-1,learning_rate=0.051,\n                      n_estimators=400,\n                      num_leaves=11,\n                      reg_alpha=2.0, \n                      reg_lambda=2.1,\n                      min_child_samples=6,\n                      min_split_gain=0.5,\n                      colsample_bytree=0.2\n                     )\nmse = []\ni=0\nfor train, test in kfold.split(X):\n    X_train = X.iloc[train]\n    y_train = Y.iloc[train]\n    X_test = X.iloc[test]\n    y_test = Y.iloc[test]\n    model.fit(X_train,y_train)\n#     model2.fit(model.predict(X_train,pred_leaf=True),y_train)\n#     y_pred = model2.predict(model.predict(X=X_test,pred_leaf=True))\n    y_pred = model.predict(X=X_test)\n    e = mean_squared_error(y_true=y_test,y_pred=y_pred)\n    mse.append(e)\n    print(e)\n    joblib.dump(filename=\"light\"+str(i),value=model)\n    i+=1\nprint(\"lightgbm\",np.mean(mse),mse)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CatBoostRegressor\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"happiness_train_complete.csv\",encoding=\"GB2312\")\ndf = df.sample(frac=1,replace=False,random_state=11)\ndf.reset_index(inplace=True)\n\ndf = df[df[\"happiness\"]>0]\nY = df[\"happiness\"]\ndf[\"survey_month\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\ndf[\"survey_day\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\ndf[\"survey_hour\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\nX = df.drop(columns=[\"id\",\"index\",\"happiness\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\n\n\nfrom sklearn.model_selection import train_test_split\nfrom catboost import Pool, CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.externals import joblib\nkfold = KFold(n_splits=15, shuffle = True, random_state= 12)\nmodel = CatBoostRegressor(colsample_bylevel=0.1,thread_count=6,silent=True,iterations=800, \n                          depth=5, \n                          learning_rate=0.051, \n                          loss_function='RMSE',\n                          l2_leaf_reg = 3)\nmse = []\ni=0\nfor train, test in kfold.split(X):\n    X_train = X.iloc[train]\n    y_train = Y.iloc[train]\n    X_test = X.iloc[test]\n    y_test = Y.iloc[test]\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    err = mean_squared_error(y_true=y_test,y_pred=y_pred)\n    mse.append(err)\n    print(err)\n    joblib.dump(filename=\"cat\"+str(i),value=model)\n    i+=1\nprint(\"catboost\",np.mean(mse),mse)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#xgboost\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"happiness_train_complete.csv\",encoding=\"GB2312\")\ndf = df.sample(frac=1,replace=False,random_state=11)\ndf.reset_index(inplace=True)\ndf = df[df[\"happiness\"]>0]\nY = df[\"happiness\"]\ndf[\"survey_month\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\ndf[\"survey_day\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\ndf[\"survey_hour\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\nX = df.drop(columns=[\"id\",\"index\",\"happiness\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=15, shuffle = True, random_state= 11)\nmodel = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.1,\n       colsample_bytree=0.971, gamma=0.11, learning_rate=0.069, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=499,\n       n_jobs=-1, nthread=50, objective='reg:linear', random_state=0,\n       reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1.0)\nmse = []\ni = 0\nfor train, test in kfold.split(X):\n    X_train = X.iloc[train]\n    y_train = Y.iloc[train]\n    X_test = X.iloc[test]\n    y_test = Y.iloc[test]\n\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    \n    xg_mse = mean_squared_error(y_true=y_test,y_pred=y_pred)\n    mse.append(xg_mse)\n    print(\"xgboost\",xg_mse)\n    joblib.dump(filename=\"xg\"+str(i),value=model)\n    i+=1\nprint(\"xgboost\",np.mean(mse),mse)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#gbdt\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"happiness_train_complete.csv\",encoding=\"GB2312\")\ndf = df.sample(frac=1,replace=False,random_state=11)\ndf.reset_index(inplace=True)\ndf = df[df[\"happiness\"]>0]\nY = df[\"happiness\"]\ndf[\"survey_month\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\ndf[\"survey_day\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\ndf[\"survey_hour\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\nX = df.drop(columns=[\"id\",\"index\",\"happiness\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=15, shuffle = True, random_state= 12)\nmodel = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.051, loss='ls', max_depth=4, max_features=10,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=600, presort='auto', random_state=3,\n             subsample=0.98, verbose=0, warm_start=False)\n\nX.fillna(-8,inplace=True)\nmse = []\ni = 0\nfor train, test in kfold.split(X):\n    X_train = X.iloc[train]\n    y_train = Y.iloc[train]\n    X_test = X.iloc[test]\n    y_test = Y.iloc[test]\n\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    gbdt_mse = mean_squared_error(y_true=y_test,y_pred=y_pred)\n    mse.append(gbdt_mse)\n    print(\"gbdt\",gbdt_mse)\n    joblib.dump(filename=\"gbdt\"+str(i),value=model)\n    i+=1\nprint(\"gbdt\",np.mean(mse),mse)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#带权平均融合CatBoostRegressor + xgboost + gbdt现有模型\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"happiness_train_complete.csv\",encoding=\"GB2312\")\ndf = df.sample(frac=1,replace=False,random_state=2000)\ndf.reset_index(inplace=True)\n\ndf = df[df[\"happiness\"]>0]\nY = df[\"happiness\"]\ndf[\"survey_month\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\ndf[\"survey_day\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\ndf[\"survey_hour\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\nX = df.drop(columns=[\"id\",\"index\",\"happiness\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\n\nfrom catboost import Pool, CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.externals import joblib\nkfold = KFold(n_splits=10, shuffle = True, random_state= 110)\ncatmse = []\nlightmse = []\nxgmse = []\ngbdtmse = []\nlrmse = []\ni = 0\nfor train, test in kfold.split(X):\n    X_train = X.iloc[train]\n    y_train = Y.iloc[train]\n    X_test = X.iloc[test]\n    y_test = Y.iloc[test]\n    \n    cat = joblib.load(filename=\"cat\"+str(i))\n    light = joblib.load(filename=\"light\"+str(i))\n    xg = joblib.load(filename=\"xg\"+str(i))\n    gbdt = joblib.load(filename=\"gbdt\"+str(i))\n    \n    catX = cat.predict(X_test)\n    cat_mse = mean_squared_error(y_true=y_test,y_pred=catX)\n    print(\"\\ncat mse:\",cat_mse)\n    catmse.append(cat_mse)\n    \n#     X_test2 = X_test.drop(columns=[\"survey_day\"])\n#     lightX = light.predict(X_test2)\n#     light_mse = mean_squared_error(y_true=y_test,y_pred=lightX)\n#     print(\"light mse:\",light_mse)\n#     lightmse.append(light_mse)\n    \n    xgX = xg.predict(X_test)\n    xg_mse = mean_squared_error(y_true=y_test,y_pred=xgX)\n    print(\"xg mse:\",xg_mse)\n    xgmse.append(xg_mse)\n    \n    X_test2 = X_test.fillna(-8)\n    gbdtX = gbdt.predict(X_test2)\n    gbdt_mse = mean_squared_error(y_true=y_test,y_pred=gbdtX)\n    print(\"gbdt mse:\",gbdt_mse)\n    gbdtmse.append(gbdt_mse)\n    \n    res = np.c_[catX,xgX,gbdtX]\n    e = np.array([1/cat_mse,1/xg_mse,1/gbdt_mse])\n    y_pred = np.sum(res*e,axis=1)/sum(e)\n    lr_mse = mean_squared_error(y_true=y_test,y_pred=y_pred)\n    print(\"lr mse:\",lr_mse)\n    lrmse.append(lr_mse)\n    \n    i+=1\n    \nprint(\"\\n\\ncatmse:\",np.mean(catmse))\n# print(\"lightmse:\",np.mean(lightmse))\nprint(\"xgmse:\",np.mean(xgmse))\nprint(\"gbdtmse:\",np.mean(gbdtmse))\nprint(\"lrmse:\",np.mean(lrmse))\n\ncat = CatBoostRegressor(colsample_bylevel=0.1,thread_count=6,silent=True,iterations=800, \n                          depth=5, \n                          learning_rate=0.051, \n                          loss_function='RMSE',\n                          l2_leaf_reg = 3)\nxg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.1,\n       colsample_bytree=0.971, gamma=0.11, learning_rate=0.069, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=499,\n       n_jobs=-1, nthread=50, objective='reg:linear', random_state=0,\n       reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1.0)\ngbdt = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.051, loss='ls', max_depth=4, max_features=10,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=600, presort='auto', random_state=3,\n             subsample=0.98, verbose=0, warm_start=False)\ncat.fit(X,Y)\nxg.fit(X,Y)\ngbdt.fit(X.fillna(-8),Y)\n    \ndf2 = pd.read_csv(\"happiness_test_complete.csv\",encoding=\"GB2312\")\ndf2[\"survey_month\"] = df2[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\ndf2[\"survey_day\"] = df2[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\ndf2[\"survey_hour\"] = df2[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\nout = df2[[\"id\"]]\nX = df2.drop(columns=[\"id\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\nX2 = X.drop(columns=[\"survey_day\"])\ncatX = cat.predict(X)\nxgX = xg.predict(X)\ngbdtX = gbdt.predict(X.fillna(-8))\nres = np.c_[catX,xgX,gbdtX]\ne = np.array([1/np.mean(catmse),1/np.mean(xgmse),1/np.mean(gbdtmse)])\ny_pred = np.sum(res*e,axis=1)/sum(e)\nout[\"happiness\"] = y_pred\nout.to_csv(\"happiness_submit.csv\",index=False)\nprint(\"done\")\nprint(e)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LR 融合CatBoostRegressor + LightGBM + xgboost + gbdt现有模型\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"happiness_train_complete.csv\",encoding=\"GB2312\")\ndf = df.sample(frac=1,replace=False,random_state=11)\ndf.reset_index(inplace=True)\n\ndf = df[df[\"happiness\"]>0]\nY = df[\"happiness\"]\ndf[\"survey_month\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\ndf[\"survey_day\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\ndf[\"survey_hour\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\nX = df.drop(columns=[\"id\",\"index\",\"happiness\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\n\nfrom catboost import Pool, CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge\nfrom sklearn.model_selection import train_test_split\nkfold = KFold(n_splits=15, shuffle = True, random_state= 12)\ncatmse = []\nlightmse = []\nxgmse = []\ngbdtmse = []\nlrmse = []\ni = 0\nfor train, test in kfold.split(X):\n    X_train = X.iloc[train]\n    y_train = Y.iloc[train]\n    X_test = X.iloc[test]\n    y_test = Y.iloc[test]\n    \n    cat = joblib.load(filename=\"cat\"+str(i))\n    light = joblib.load(filename=\"light\"+str(i))\n    xg = joblib.load(filename=\"xg\"+str(i))\n    gbdt = joblib.load(filename=\"gbdt\"+str(i))\n    \n    catX = cat.predict(X_test)\n    cat_mse = mean_squared_error(y_true=y_test,y_pred=catX)\n    print(\"\\ncat mse:\",cat_mse)\n    catmse.append(cat_mse)\n\n    lightX = light.predict(X_test)\n    light_mse = mean_squared_error(y_true=y_test,y_pred=lightX)\n    print(\"light mse:\",light_mse)\n    lightmse.append(light_mse)\n    \n    xgX = xg.predict(X_test)\n    xg_mse = mean_squared_error(y_true=y_test,y_pred=xgX)\n    print(\"xg mse:\",xg_mse)\n    xgmse.append(xg_mse)\n    \n    gbdtX = gbdt.predict(X_test.fillna(-8))\n    gbdt_mse = mean_squared_error(y_true=y_test,y_pred=gbdtX)\n    print(\"gbdt mse:\",gbdt_mse)\n    gbdtmse.append(gbdt_mse)\n    \n    res = np.c_[catX,lightX,xgX,gbdtX]\n    lr = Ridge(fit_intercept=False, alpha=75)\n    lr.fit(res,y_test)\n    print(lr.coef_)\n\n    y_pred = lr.predict(res)\n    lr_mse = mean_squared_error(y_true=y_test,y_pred=y_pred)\n    print(\"lr mse:\",lr_mse)\n    lrmse.append(lr_mse)\n    joblib.dump(filename=\"lr\"+str(i),value=lr)\n    i+=1\n    \nprint(\"\\ncatmse:\",np.mean(catmse))\nprint(\"\\n\\nlightmse:\",np.mean(lightmse))\nprint(\"xgmse:\",np.mean(xgmse))\nprint(\"gbdtmse:\",np.mean(gbdtmse))\nprint(\"lrmse:\",np.mean(lrmse))\n\n    \ndf2 = pd.read_csv(\"happiness_test_complete.csv\",encoding=\"GB2312\")\ndf2[\"survey_month\"] = df2[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\ndf2[\"survey_day\"] = df2[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\ndf2[\"survey_hour\"] = df2[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\nout = df2[[\"id\"]]\nX = df2.drop(columns=[\"id\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\nprediction = []\nfor i in range(15):\n    cat = joblib.load(filename=\"cat\"+str(i))\n    light = joblib.load(filename=\"light\"+str(i))\n    xg = joblib.load(filename=\"xg\"+str(i))\n    gbdt = joblib.load(filename=\"gbdt\"+str(i))\n    lr = joblib.load(filename=\"lr\"+str(i))\n    \n    catX = cat.predict(X)\n    lightX = light.predict(X)\n    xgX = xg.predict(X)\n    gbdtX = gbdt.predict(X.fillna(-8))\n    res = np.c_[catX,lightX,xgX,gbdtX]\n    prediction.append(lr.predict(res))\n    \na = np.array(prediction)\ndef cut(arr):\n    arr2 = []\n    for x in arr:\n        if x<1:\n            arr2.append(1)\n        elif x>5:\n            arr2.append(5)\n        else :\n            arr2.append(x)\n    return arr2\nout[\"happiness\"] = np.mean(np.array(prediction),axis=0)\nout.to_csv(\"happiness_submit.csv\",index=False)\nprint(\"done\")\n\n\n# out[\"happiness\"] = cut(np.sum((1/np.array(lrmse)*a.T),axis=1)/np.sum(1/np.array(lrmse)))\n# out.to_csv(\"happiness_submit.csv\",index=False)\nprint(\"done\")","metadata":{},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}}}