{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线上效果0.4719，21名，受邀请分享一下思路~\n",
    "\n",
    "总体思路：分别使用LightGBM，xgboost，gbdt，catboost建立多个个体学习器（加入bagging的策略，对数据随机采样），对最终学习器的输出使用岭回归进一步提升精度。代码如下。\n",
    "\n",
    "改进点：\n",
    "1.可以在详细分析一下字段，可以考虑对字段进行特殊处理。\n",
    "2.超参数还可以调，我没有使用网格搜索，只是简单的进行的调参。\n",
    "3.如果单纯为了提高精度，可以更高随机种子，多试几次\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"happiness_train_complete.csv\",encoding=\"GB2312\")\n",
    "df = df.sample(frac=1,replace=False,random_state=11)\n",
    "df.reset_index(inplace=True)\n",
    "df = df[df[\"happiness\"]>0]\n",
    "Y = df[\"happiness\"]\n",
    "df[\"survey_month\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\n",
    "df[\"survey_day\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\n",
    "df[\"survey_hour\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\n",
    "X = df.drop(columns=[\"id\",\"index\",\"happiness\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=15, shuffle = True, random_state= 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(n_jobs=-1,learning_rate=0.051,\n",
    "                      n_estimators=400,\n",
    "                      num_leaves=11,\n",
    "                      reg_alpha=2.0, \n",
    "                      reg_lambda=2.1,\n",
    "                      min_child_samples=6,\n",
    "                      min_split_gain=0.5,\n",
    "                      colsample_bytree=0.2\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5246388446214709\n",
      "0.44853289149980635\n",
      "0.39798888520938375\n",
      "0.4653840765647241\n",
      "0.4198606821280977\n",
      "0.4122033421642622\n",
      "0.502844325280201\n",
      "0.4939943554900641\n",
      "0.45983459406105315\n",
      "0.49773527223459996\n",
      "0.44428059718587704\n",
      "0.4819013944449417\n",
      "0.4556318435524804\n",
      "0.46869886997155735\n",
      "0.4244474502100046\n",
      "lightgbm 0.4598651616412349 [0.5246388446214709, 0.44853289149980635, 0.39798888520938375, 0.4653840765647241, 0.4198606821280977, 0.4122033421642622, 0.502844325280201, 0.4939943554900641, 0.45983459406105315, 0.49773527223459996, 0.44428059718587704, 0.4819013944449417, 0.4556318435524804, 0.46869886997155735, 0.4244474502100046]\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "i=0\n",
    "for train, test in kfold.split(X):#第一个是训练集的索引，第二个是验证集的索引。\n",
    "    X_train = X.iloc[train]\n",
    "    y_train = Y.iloc[train]\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = Y.iloc[test]\n",
    "    model.fit(X_train,y_train)\n",
    "#     model2.fit(model.predict(X_train,pred_leaf=True),y_train)\n",
    "#     y_pred = model2.predict(model.predict(X=X_test,pred_leaf=True))\n",
    "    y_pred = model.predict(X=X_test)\n",
    "    e = mean_squared_error(y_true=y_test,y_pred=y_pred)\n",
    "    mse.append(e)\n",
    "    print(e)\n",
    "    joblib.dump(filename=\"light\"+str(i),value=model)\n",
    "    i+=1\n",
    "print(\"lightgbm\",np.mean(mse),mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5140745902446223\n",
      "0.44963717420374527\n",
      "0.40583414150925673\n",
      "0.46129771045939694\n",
      "0.4155310816111115\n",
      "0.40941501701181154\n",
      "0.4998116621189004\n",
      "0.48265208770060597\n",
      "0.45761449468465004\n",
      "0.4895659400203275\n",
      "0.4398279303183797\n",
      "0.48314758270880803\n",
      "0.4551596579865996\n",
      "0.46339945766808416\n",
      "0.4220552064306169\n",
      "catboost 0.4566015823117944 [0.5140745902446223, 0.44963717420374527, 0.40583414150925673, 0.46129771045939694, 0.4155310816111115, 0.40941501701181154, 0.4998116621189004, 0.48265208770060597, 0.45761449468465004, 0.4895659400203275, 0.4398279303183797, 0.48314758270880803, 0.4551596579865996, 0.46339945766808416, 0.4220552064306169]\n"
     ]
    }
   ],
   "source": [
    "#CatBoostRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"happiness_train_complete.csv\",encoding=\"GB2312\")\n",
    "df = df.sample(frac=1,replace=False,random_state=11)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df = df[df[\"happiness\"]>0]\n",
    "Y = df[\"happiness\"]\n",
    "df[\"survey_month\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\n",
    "df[\"survey_day\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\n",
    "df[\"survey_hour\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\n",
    "X = df.drop(columns=[\"id\",\"index\",\"happiness\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.externals import joblib\n",
    "kfold = KFold(n_splits=15, shuffle = True, random_state= 12)\n",
    "model = CatBoostRegressor(colsample_bylevel=0.1,thread_count=6,silent=True,iterations=800, \n",
    "                          depth=5, \n",
    "                          learning_rate=0.051, \n",
    "                          loss_function='RMSE',\n",
    "                          l2_leaf_reg = 3)\n",
    "mse = []\n",
    "i=0\n",
    "for train, test in kfold.split(X):\n",
    "    X_train = X.iloc[train]\n",
    "    y_train = Y.iloc[train]\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = Y.iloc[test]\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    err = mean_squared_error(y_true=y_test,y_pred=y_pred)\n",
    "    mse.append(err)\n",
    "    print(err)\n",
    "    joblib.dump(filename=\"cat\"+str(i),value=model)\n",
    "    i+=1\n",
    "print(\"catboost\",np.mean(mse),mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost 0.45450514377237855\n",
      "xgboost 0.4704459080976553\n",
      "xgboost 0.45398107906457325\n",
      "xgboost 0.4752842442418985\n",
      "xgboost 0.44166640422869086\n",
      "xgboost 0.4740424619970855\n",
      "xgboost 0.414644665043313\n",
      "xgboost 0.451792770308976\n",
      "xgboost 0.47684027202123286\n",
      "xgboost 0.49329761555264395\n",
      "xgboost 0.4755900949442348\n",
      "xgboost 0.4366128914483463\n",
      "xgboost 0.5073443590751492\n",
      "xgboost 0.42703606159588203\n",
      "xgboost 0.4782073499962756\n",
      "xgboost 0.46208608809255575 [0.45450514377237855, 0.4704459080976553, 0.45398107906457325, 0.4752842442418985, 0.44166640422869086, 0.4740424619970855, 0.414644665043313, 0.451792770308976, 0.47684027202123286, 0.49329761555264395, 0.4755900949442348, 0.4366128914483463, 0.5073443590751492, 0.42703606159588203, 0.4782073499962756]\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"happiness_train_complete.csv\",encoding=\"GB2312\")\n",
    "df = df.sample(frac=1,replace=False,random_state=11)\n",
    "df.reset_index(inplace=True)\n",
    "df = df[df[\"happiness\"]>0]\n",
    "Y = df[\"happiness\"]\n",
    "df[\"survey_month\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\n",
    "df[\"survey_day\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\n",
    "df[\"survey_hour\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\n",
    "X = df.drop(columns=[\"id\",\"index\",\"happiness\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\n",
    "\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=15, shuffle = True, random_state= 11)\n",
    "model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.1,\n",
    "       colsample_bytree=0.971, gamma=0.11, learning_rate=0.069, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=499,\n",
    "       n_jobs=-1, nthread=50, objective='reg:linear', random_state=0,\n",
    "       reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1.0)\n",
    "\n",
    "mse = []\n",
    "i = 0\n",
    "for train, test in kfold.split(X):\n",
    "    X_train = X.iloc[train]\n",
    "    y_train = Y.iloc[train]\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = Y.iloc[test]\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    xg_mse = mean_squared_error(y_true=y_test,y_pred=y_pred)\n",
    "    mse.append(xg_mse)\n",
    "    print(\"xgboost\",xg_mse)\n",
    "    joblib.dump(filename=\"xg\"+str(i),value=model)\n",
    "    i+=1\n",
    "print(\"xgboost\",np.mean(mse),mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbdt 0.5189500888507569\n",
      "gbdt 0.4570418891557324\n",
      "gbdt 0.4033749775774016\n",
      "gbdt 0.45943222208389095\n",
      "gbdt 0.4286129037296931\n",
      "gbdt 0.42570638271447414\n",
      "gbdt 0.5043808541694681\n",
      "gbdt 0.5032374595207704\n",
      "gbdt 0.45540367596333403\n",
      "gbdt 0.49633563078272347\n",
      "gbdt 0.45106702861984144\n",
      "gbdt 0.4972331370414904\n",
      "gbdt 0.4568532289923147\n",
      "gbdt 0.46964687347159\n",
      "gbdt 0.43481614328190643\n",
      "gbdt 0.4641394997303593 [0.5189500888507569, 0.4570418891557324, 0.4033749775774016, 0.45943222208389095, 0.4286129037296931, 0.42570638271447414, 0.5043808541694681, 0.5032374595207704, 0.45540367596333403, 0.49633563078272347, 0.45106702861984144, 0.4972331370414904, 0.4568532289923147, 0.46964687347159, 0.43481614328190643]\n"
     ]
    }
   ],
   "source": [
    "#gbdt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"happiness_train_complete.csv\",encoding=\"GB2312\")\n",
    "df = df.sample(frac=1,replace=False,random_state=11)\n",
    "df.reset_index(inplace=True)\n",
    "df = df[df[\"happiness\"]>0]\n",
    "Y = df[\"happiness\"]\n",
    "df[\"survey_month\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\n",
    "df[\"survey_day\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\n",
    "df[\"survey_hour\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\n",
    "X = df.drop(columns=[\"id\",\"index\",\"happiness\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=15, shuffle = True, random_state= 12)\n",
    "model = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "             learning_rate=0.051, loss='ls', max_depth=4, max_features=10,\n",
    "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "             min_impurity_split=None, min_samples_leaf=1,\n",
    "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "             n_estimators=600, presort='auto', random_state=3,\n",
    "             subsample=0.98, verbose=0, warm_start=False)\n",
    "\n",
    "X.fillna(-8,inplace=True)\n",
    "\n",
    "mse = []\n",
    "i = 0\n",
    "for train, test in kfold.split(X):\n",
    "    X_train = X.iloc[train]\n",
    "    y_train = Y.iloc[train]\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = Y.iloc[test]\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    gbdt_mse = mean_squared_error(y_true=y_test,y_pred=y_pred)\n",
    "    mse.append(gbdt_mse)\n",
    "    print(\"gbdt\",gbdt_mse)\n",
    "    joblib.dump(filename=\"gbdt\"+str(i),value=model)\n",
    "    i+=1\n",
    "print(\"gbdt\",np.mean(mse),mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cat mse: 0.3068473087722804\n",
      "xg mse: 0.3594664784459855\n",
      "gbdt mse: 0.284963304317945\n",
      "lr mse: 0.3099544960184681\n",
      "\n",
      "cat mse: 0.26379334475643545\n",
      "xg mse: 0.3090892975232685\n",
      "gbdt mse: 0.2428776859214876\n",
      "lr mse: 0.26508254457664676\n",
      "\n",
      "cat mse: 0.3122819689309576\n",
      "xg mse: 0.36448487487557585\n",
      "gbdt mse: 0.2899090649776326\n",
      "lr mse: 0.31457046922213133\n",
      "\n",
      "cat mse: 0.31720119359048826\n",
      "xg mse: 0.37212291504245154\n",
      "gbdt mse: 0.2859330265818493\n",
      "lr mse: 0.3166726800712165\n",
      "\n",
      "cat mse: 0.2981851002570271\n",
      "xg mse: 0.35301986484474096\n",
      "gbdt mse: 0.2716722895520111\n",
      "lr mse: 0.2996516041696275\n",
      "\n",
      "cat mse: 0.27755281034439633\n",
      "xg mse: 0.31740095174973676\n",
      "gbdt mse: 0.2581780452897828\n",
      "lr mse: 0.2787925340892307\n",
      "\n",
      "cat mse: 0.32922390966018006\n",
      "xg mse: 0.37127795635373706\n",
      "gbdt mse: 0.29988440926202026\n",
      "lr mse: 0.3259930877693204\n",
      "\n",
      "cat mse: 0.29257941335360815\n",
      "xg mse: 0.3289029414071347\n",
      "gbdt mse: 0.26512869295338437\n",
      "lr mse: 0.28914886675897644\n",
      "\n",
      "cat mse: 0.29058454455327043\n",
      "xg mse: 0.33871833328434314\n",
      "gbdt mse: 0.26081918866829057\n",
      "lr mse: 0.28828425028115856\n",
      "\n",
      "cat mse: 0.29601725053397243\n",
      "xg mse: 0.33794355461256664\n",
      "gbdt mse: 0.27467443248149637\n",
      "lr mse: 0.29561164753157293\n",
      "\n",
      "\n",
      "catmse: 0.2984266844752616\n",
      "xgmse: 0.3452427168139541\n",
      "gbdtmse: 0.27340401400059\n",
      "lrmse: 0.2983762180488349\n",
      "done\n",
      "[3.35090678 2.89651295 3.65759078]\n"
     ]
    }
   ],
   "source": [
    "#带权平均融合CatBoostRegressor + xgboost + gbdt现有模型\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"happiness_train_complete.csv\",encoding=\"GB2312\")\n",
    "df = df.sample(frac=1,replace=False,random_state=2000)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df = df[df[\"happiness\"]>0]\n",
    "Y = df[\"happiness\"]\n",
    "df[\"survey_month\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\n",
    "df[\"survey_day\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\n",
    "df[\"survey_hour\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\n",
    "X = df.drop(columns=[\"id\",\"index\",\"happiness\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\n",
    "\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.externals import joblib\n",
    "kfold = KFold(n_splits=10, shuffle = True, random_state= 110)\n",
    "catmse = []\n",
    "lightmse = []\n",
    "xgmse = []\n",
    "gbdtmse = []\n",
    "lrmse = []\n",
    "i = 0\n",
    "for train, test in kfold.split(X):\n",
    "    X_train = X.iloc[train]\n",
    "    y_train = Y.iloc[train]\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = Y.iloc[test]\n",
    "    \n",
    "    cat = joblib.load(filename=\"cat\"+str(i))\n",
    "    light = joblib.load(filename=\"light\"+str(i))\n",
    "    xg = joblib.load(filename=\"xg\"+str(i))\n",
    "    gbdt = joblib.load(filename=\"gbdt\"+str(i))\n",
    "    \n",
    "    catX = cat.predict(X_test)\n",
    "    cat_mse = mean_squared_error(y_true=y_test,y_pred=catX)\n",
    "    print(\"\\ncat mse:\",cat_mse)\n",
    "    catmse.append(cat_mse)\n",
    "    \n",
    "#     X_test2 = X_test.drop(columns=[\"survey_day\"])\n",
    "#     lightX = light.predict(X_test2)\n",
    "#     light_mse = mean_squared_error(y_true=y_test,y_pred=lightX)\n",
    "#     print(\"light mse:\",light_mse)\n",
    "#     lightmse.append(light_mse)\n",
    "    \n",
    "    xgX = xg.predict(X_test)\n",
    "    xg_mse = mean_squared_error(y_true=y_test,y_pred=xgX)\n",
    "    print(\"xg mse:\",xg_mse)\n",
    "    xgmse.append(xg_mse)\n",
    "    \n",
    "    X_test2 = X_test.fillna(-8)\n",
    "    gbdtX = gbdt.predict(X_test2)\n",
    "    gbdt_mse = mean_squared_error(y_true=y_test,y_pred=gbdtX)\n",
    "    print(\"gbdt mse:\",gbdt_mse)\n",
    "    gbdtmse.append(gbdt_mse)\n",
    "    \n",
    "    res = np.c_[catX,xgX,gbdtX]\n",
    "    e = np.array([1/cat_mse,1/xg_mse,1/gbdt_mse])\n",
    "    y_pred = np.sum(res*e,axis=1)/sum(e)\n",
    "    lr_mse = mean_squared_error(y_true=y_test,y_pred=y_pred)\n",
    "    print(\"lr mse:\",lr_mse)\n",
    "    lrmse.append(lr_mse)\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "print(\"\\n\\ncatmse:\",np.mean(catmse))\n",
    "# print(\"lightmse:\",np.mean(lightmse))\n",
    "print(\"xgmse:\",np.mean(xgmse))\n",
    "print(\"gbdtmse:\",np.mean(gbdtmse))\n",
    "print(\"lrmse:\",np.mean(lrmse))\n",
    "\n",
    "cat = CatBoostRegressor(colsample_bylevel=0.1,thread_count=6,silent=True,iterations=800, \n",
    "                          depth=5, \n",
    "                          learning_rate=0.051, \n",
    "                          loss_function='RMSE',\n",
    "                          l2_leaf_reg = 3)\n",
    "xg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.1,\n",
    "       colsample_bytree=0.971, gamma=0.11, learning_rate=0.069, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=499,\n",
    "       n_jobs=-1, nthread=50, objective='reg:linear', random_state=0,\n",
    "       reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1.0)\n",
    "gbdt = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "             learning_rate=0.051, loss='ls', max_depth=4, max_features=10,\n",
    "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "             min_impurity_split=None, min_samples_leaf=1,\n",
    "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "             n_estimators=600, presort='auto', random_state=3,\n",
    "             subsample=0.98, verbose=0, warm_start=False)\n",
    "cat.fit(X,Y)\n",
    "xg.fit(X,Y)\n",
    "gbdt.fit(X.fillna(-8),Y)\n",
    "    \n",
    "df2 = pd.read_csv(\"happiness_test_complete.csv\",encoding=\"GB2312\")\n",
    "df2[\"survey_month\"] = df2[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\n",
    "df2[\"survey_day\"] = df2[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\n",
    "df2[\"survey_hour\"] = df2[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\n",
    "\n",
    "out = df2[[\"id\"]]\n",
    "X = df2.drop(columns=[\"id\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\n",
    "X2 = X.drop(columns=[\"survey_day\"])\n",
    "catX = cat.predict(X)\n",
    "xgX = xg.predict(X)\n",
    "gbdtX = gbdt.predict(X.fillna(-8))\n",
    "\n",
    "res = np.c_[catX,xgX,gbdtX]\n",
    "e = np.array([1/np.mean(catmse),1/np.mean(xgmse),1/np.mean(gbdtmse)])\n",
    "y_pred = np.sum(res*e,axis=1)/sum(e)\n",
    "out[\"happiness\"] = y_pred\n",
    "out.to_csv(\"happiness_submit.csv\",index=False)\n",
    "print(\"done\")\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cat mse: 0.5140745902446223\n",
      "light mse: 0.5246388446214709\n",
      "xg mse: 0.3949933979705159\n",
      "gbdt mse: 0.5189500888507569\n",
      "[0.15692732 0.130091   0.56216864 0.1499844 ]\n",
      "lr mse: 0.441715467691966\n",
      "\n",
      "cat mse: 0.44963717420374527\n",
      "light mse: 0.44853289149980635\n",
      "xg mse: 0.34921170255919454\n",
      "gbdt mse: 0.4570418891557324\n",
      "[0.169487   0.17313728 0.51354472 0.14734901]\n",
      "lr mse: 0.392826862591736\n",
      "\n",
      "cat mse: 0.40583414150925673\n",
      "light mse: 0.39798888520938375\n",
      "xg mse: 0.3192823411725259\n",
      "gbdt mse: 0.4033749775774016\n",
      "[0.16202939 0.19026981 0.46212397 0.18036186]\n",
      "lr mse: 0.3578575641042594\n",
      "\n",
      "cat mse: 0.46129771045939694\n",
      "light mse: 0.4653840765647241\n",
      "xg mse: 0.3557698168444744\n",
      "gbdt mse: 0.45943222208389095\n",
      "[0.15867871 0.15014093 0.52070624 0.17347199]\n",
      "lr mse: 0.399966975754544\n",
      "\n",
      "cat mse: 0.4155310816111115\n",
      "light mse: 0.4198606821280977\n",
      "xg mse: 0.33401968129637316\n",
      "gbdt mse: 0.4286129037296931\n",
      "[0.19045141 0.17835247 0.47440746 0.15176293]\n",
      "lr mse: 0.37433437332477126\n",
      "\n",
      "cat mse: 0.40941501701181154\n",
      "light mse: 0.4122033421642622\n",
      "xg mse: 0.3104125039599635\n",
      "gbdt mse: 0.42570638271447414\n",
      "[0.17886372 0.17269412 0.51999735 0.13702093]\n",
      "lr mse: 0.3520500436782725\n",
      "\n",
      "cat mse: 0.4998116621189004\n",
      "light mse: 0.502844325280201\n",
      "xg mse: 0.36918234855236065\n",
      "gbdt mse: 0.5043808541694681\n",
      "[0.14442875 0.13658049 0.58499052 0.13382132]\n",
      "lr mse: 0.4176146380982895\n",
      "\n",
      "cat mse: 0.48265208770060597\n",
      "light mse: 0.4939943554900641\n",
      "xg mse: 0.37212283083906955\n",
      "gbdt mse: 0.5032374595207704\n",
      "[0.17475542 0.14206194 0.55450562 0.12280173]\n",
      "lr mse: 0.41828362864511986\n",
      "\n",
      "cat mse: 0.45761449468465004\n",
      "light mse: 0.45983459406105315\n",
      "xg mse: 0.33965884645468014\n",
      "gbdt mse: 0.45540367596333403\n",
      "[0.14894828 0.14267426 0.54818015 0.15988788]\n",
      "lr mse: 0.3860423164115492\n",
      "\n",
      "cat mse: 0.4895659400203275\n",
      "light mse: 0.49773527223459996\n",
      "xg mse: 0.3719207244168279\n",
      "gbdt mse: 0.49633563078272347\n",
      "[0.16120809 0.13500227 0.56004155 0.14500386]\n",
      "lr mse: 0.41900282361235597\n",
      "\n",
      "cat mse: 0.4398279303183797\n",
      "light mse: 0.44428059718587704\n",
      "xg mse: 0.326344872565609\n",
      "gbdt mse: 0.45106702861984144\n",
      "[0.16072263 0.15376332 0.5479808  0.13595033]\n",
      "lr mse: 0.3717244810599866\n",
      "\n",
      "cat mse: 0.48314758270880803\n",
      "light mse: 0.4819013944449417\n",
      "xg mse: 0.3701659821651882\n",
      "gbdt mse: 0.4972331370414904\n",
      "[0.16452977 0.17212734 0.54768125 0.12695985]\n",
      "lr mse: 0.41389036669658097\n",
      "\n",
      "cat mse: 0.4551596579865996\n",
      "light mse: 0.4556318435524804\n",
      "xg mse: 0.34916555587475895\n",
      "gbdt mse: 0.4568532289923147\n",
      "[0.15596857 0.15960594 0.51815487 0.15614979]\n",
      "lr mse: 0.392563352792169\n",
      "\n",
      "cat mse: 0.46339945766808416\n",
      "light mse: 0.46869886997155735\n",
      "xg mse: 0.3594686242399743\n",
      "gbdt mse: 0.46964687347159\n",
      "[0.16628703 0.15097304 0.52319701 0.15105458]\n",
      "lr mse: 0.40440341392349005\n",
      "\n",
      "cat mse: 0.4220552064306169\n",
      "light mse: 0.4244474502100046\n",
      "xg mse: 0.31839021940451495\n",
      "gbdt mse: 0.43481614328190643\n",
      "[0.16931471 0.16438659 0.52586618 0.13520741]\n",
      "lr mse: 0.36270028344885147\n",
      "\n",
      "catmse: 0.4566015823117944\n",
      "\n",
      "\n",
      "lightmse: 0.4598651616412349\n",
      "xgmse: 0.3493406298877354\n",
      "gbdtmse: 0.4641394997303593\n",
      "lrmse: 0.3936651061222628\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#LR 融合CatBoostRegressor + LightGBM + xgboost + gbdt现有模型\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"happiness_train_complete.csv\",encoding=\"GB2312\")\n",
    "df = df.sample(frac=1,replace=False,random_state=11)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df = df[df[\"happiness\"]>0]\n",
    "Y = df[\"happiness\"]\n",
    "df[\"survey_month\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\n",
    "df[\"survey_day\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\n",
    "df[\"survey_hour\"] = df[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\n",
    "X = df.drop(columns=[\"id\",\"index\",\"happiness\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\n",
    "\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "kfold = KFold(n_splits=15, shuffle = True, random_state= 12)\n",
    "catmse = []\n",
    "lightmse = []\n",
    "xgmse = []\n",
    "gbdtmse = []\n",
    "lrmse = []\n",
    "i = 0\n",
    "for train, test in kfold.split(X):\n",
    "    X_train = X.iloc[train]\n",
    "    y_train = Y.iloc[train]\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = Y.iloc[test]\n",
    "    \n",
    "    cat = joblib.load(filename=\"cat\"+str(i))\n",
    "    light = joblib.load(filename=\"light\"+str(i))\n",
    "    xg = joblib.load(filename=\"xg\"+str(i))\n",
    "    gbdt = joblib.load(filename=\"gbdt\"+str(i))\n",
    "    \n",
    "    catX = cat.predict(X_test)\n",
    "    cat_mse = mean_squared_error(y_true=y_test,y_pred=catX)\n",
    "    print(\"\\ncat mse:\",cat_mse)\n",
    "    catmse.append(cat_mse)\n",
    "\n",
    "    lightX = light.predict(X_test)\n",
    "    light_mse = mean_squared_error(y_true=y_test,y_pred=lightX)\n",
    "    print(\"light mse:\",light_mse)\n",
    "    lightmse.append(light_mse)\n",
    "    \n",
    "    xgX = xg.predict(X_test)\n",
    "    xg_mse = mean_squared_error(y_true=y_test,y_pred=xgX)\n",
    "    print(\"xg mse:\",xg_mse)\n",
    "    xgmse.append(xg_mse)\n",
    "    \n",
    "    gbdtX = gbdt.predict(X_test.fillna(-8))\n",
    "    gbdt_mse = mean_squared_error(y_true=y_test,y_pred=gbdtX)\n",
    "    print(\"gbdt mse:\",gbdt_mse)\n",
    "    gbdtmse.append(gbdt_mse)\n",
    "    \n",
    "    res = np.c_[catX,lightX,xgX,gbdtX]\n",
    "    lr = Ridge(fit_intercept=False, alpha=75)\n",
    "    lr.fit(res,y_test)\n",
    "    print(lr.coef_)\n",
    "\n",
    "    y_pred = lr.predict(res)\n",
    "    lr_mse = mean_squared_error(y_true=y_test,y_pred=y_pred)\n",
    "    print(\"lr mse:\",lr_mse)\n",
    "    lrmse.append(lr_mse)\n",
    "    joblib.dump(filename=\"lr\"+str(i),value=lr)\n",
    "    i+=1\n",
    "    \n",
    "print(\"\\ncatmse:\",np.mean(catmse))\n",
    "print(\"\\n\\nlightmse:\",np.mean(lightmse))\n",
    "print(\"xgmse:\",np.mean(xgmse))\n",
    "print(\"gbdtmse:\",np.mean(gbdtmse))\n",
    "print(\"lrmse:\",np.mean(lrmse))\n",
    "\n",
    "    \n",
    "df2 = pd.read_csv(\"happiness_test_complete.csv\",encoding=\"GB2312\")\n",
    "df2[\"survey_month\"] = df2[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[1]).astype(\"int64\")\n",
    "df2[\"survey_day\"] = df2[\"survey_time\"].map(lambda line:line.split(\" \")[0].split(\"/\")[2]).astype(\"int64\")\n",
    "df2[\"survey_hour\"] = df2[\"survey_time\"].map(lambda line:line.split(\" \")[1].split(\":\")[0]).astype(\"int64\")\n",
    "out = df2[[\"id\"]]\n",
    "X = df2.drop(columns=[\"id\",\"survey_time\",\"edu_other\",\"property_other\",\"invest_other\"])\n",
    "prediction = []\n",
    "\n",
    "for i in range(15):\n",
    "    cat = joblib.load(filename=\"cat\"+str(i))\n",
    "    light = joblib.load(filename=\"light\"+str(i))\n",
    "    xg = joblib.load(filename=\"xg\"+str(i))\n",
    "    gbdt = joblib.load(filename=\"gbdt\"+str(i))\n",
    "    lr = joblib.load(filename=\"lr\"+str(i))\n",
    "    \n",
    "    catX = cat.predict(X)\n",
    "    lightX = light.predict(X)\n",
    "    xgX = xg.predict(X)\n",
    "    gbdtX = gbdt.predict(X.fillna(-8))\n",
    "    res = np.c_[catX,lightX,xgX,gbdtX]\n",
    "    prediction.append(lr.predict(res))\n",
    "    \n",
    "a = np.array(prediction)\n",
    "def cut(arr):\n",
    "    arr2 = []\n",
    "    for x in arr:\n",
    "        if x<1:\n",
    "            arr2.append(1)\n",
    "        elif x>5:\n",
    "            arr2.append(5)\n",
    "        else :\n",
    "            arr2.append(x)\n",
    "    return arr2\n",
    "out[\"happiness\"] = np.mean(np.array(prediction),axis=0)\n",
    "out.to_csv(\"happiness_submit.csv\",index=False)\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "# out[\"happiness\"] = cut(np.sum((1/np.array(lrmse)*a.T),axis=1)/np.sum(1/np.array(lrmse)))\n",
    "# out.to_csv(\"happiness_submit.csv\",index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Miniconda-sklearn]",
   "language": "python",
   "name": "conda-env-Miniconda-sklearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
